\documentclass{ctexart}

\usepackage[top=1.5cm, bottom=1.5cm, left=2.0cm, right=2.0cm]{geometry}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{graphicx}

\pagestyle{plain}
\ctexset{tablename = {Table}}

\newcommand{\code}[1]{\texttt{\detokenize{#1}}} 

\lstset{
    basicstyle=\ttfamily\small,
    numbers=left,
    numberstyle=\tiny,
    frame=single,
    breaklines=true,
    captionpos=b,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{red},
    showstringspaces=false
}

\newcounter{cnt}
\theoremstyle{definition} \newtheorem{problem}[cnt]{Problem}
\AtBeginEnvironment{problem}{\setlength{\parindent}{0pt}}
\newtheorem*{sol}{Solution}

\newlist{questions}{enumerate}{1}
\setlist[questions]{label=(\alph*), leftmargin=2.2em, itemsep=0.6em}
\newlist{subquestions}{enumerate}{1}
\setlist[subquestions]{label=(\alph*), leftmargin=2.2em, itemsep=0.4em}

\newenvironment{solution}{\begin{sol}\end{sol}\setlength{\parindent}{0pt}\begin{questions}}{
    \end{questions}\vspace{2.5em}}
\newenvironment{solution*}{\begin{sol}\end{sol}}{\vspace{2.5em}}

\title{CS336 Assignment 2: Systems and Parallelism}
\author{袁奕}

\begin{document}

\maketitle

\section{Optimizing Attention with FlashAttention-2}

\begin{problem}[\code{benchmarking_script}]
End-to-End Benchmarking (4 points)
\begin{questions}
    \item Write a script to perform basic end-to-end benchmarking of the forward and backward passes in your model. Your script should support initializing a model from hyperparameters, generating random batches, and timing $n$ steps after $w$ warm-up steps. Use \code{torch.cuda.synchronize()} after each step.
    \item Time the forward and backward passes for the model sizes described in Table 1 (Small, Medium, Large, XL, 2.7B). Use 5 warmup steps and 10 measurement steps. Report the average and standard deviation.
    \item Repeat the analysis without warm-up steps, and with only 1 or 2 warm-up steps. Why do the results differ?
\end{questions}
\end{problem}
\begin{solution}
    \item \url{https://github.com/yuanyi-350/llm-from-scratch-assignment2-systems/blob/master/cs336_systems/benchmark.py}
    
    \item 
    
    \begin{lstlisting}
stu2400010766@lfs-dev:~/cs336-2$ uv run python -m cs336_systems.benchmark \ 
    --model_size small --context_length 128 --mode forward --n_steps 5 --profile_memory
Running on cuda
Monkey patching scaled_dot_product_attention with NVTX annotated version...
Using Full Precision (FP32)
Warming up for 5 steps...
Measuring for 5 steps...
Starting memory recording...
Peak Memory Usage: 546.23 MB
Memory snapshot saved to memory_snapshot_small_128_forward.pickle
Model: small, Context: 128, Mode: forward, BF16: False
Average Time: 0.0158 s
Std Dev: 0.0005 s

stu2400010766@lfs-dev:~/cs336-2$ uv run python -m cs336_systems.benchmark \ 
    --model_size medium --context_length 128 --mode forward --n_steps 5 --profile_memory
Running on cuda
Monkey patching scaled_dot_product_attention with NVTX annotated version...
Using Full Precision (FP32)
Warming up for 5 steps...
Measuring for 5 steps...
Starting memory recording...
Peak Memory Usage: 1676.35 MB
Memory snapshot saved to memory_snapshot_medium_128_forward.pickle
Model: medium, Context: 128, Mode: forward, BF16: False
Average Time: 0.0313 s
Std Dev: 0.0007 s

stu2400010766@lfs-dev:~/cs336-2$ uv run python -m cs336_systems.benchmark \ 
    --model_size large --context_length 128 --mode forward --n_steps 5 --profile_memory
Running on cuda
Monkey patching scaled_dot_product_attention with NVTX annotated version...
Using Full Precision (FP32)
Warming up for 5 steps...
Measuring for 5 steps...
Starting memory recording...
Peak Memory Usage: 3874.17 MB
Memory snapshot saved to memory_snapshot_large_128_forward.pickle
Model: large, Context: 128, Mode: forward, BF16: False
Average Time: 0.0468 s
Std Dev: 0.0009 s

stu2400010766@lfs-dev:~/cs336-2$ uv run python -m cs336_systems.benchmark \ 
    --model_size xl --context_length 128 --mode forward --n_steps 5 --profile_memory
Running on cuda
Monkey patching scaled_dot_product_attention with NVTX annotated version...
Using Full Precision (FP32)
Warming up for 5 steps...
Measuring for 5 steps...
Starting memory recording...
Peak Memory Usage: 7882.78 MB
Memory snapshot saved to memory_snapshot_xl_128_forward.pickle
Model: xl, Context: 128, Mode: forward, BF16: False
Average Time: 0.0616 s
Std Dev: 0.0007 s

stu2400010766@lfs-dev:~/cs336-2$ uv run python -m cs336_systems.benchmark \ 
    --model_size 2.7B --context_length 128 --mode forward --n_steps 5 --profile_memory
Running on cuda
Monkey patching scaled_dot_product_attention with NVTX annotated version...
Using Full Precision (FP32)
Warming up for 5 steps...
Measuring for 5 steps...
Starting memory recording...
Peak Memory Usage: 13232.80 MB
Memory snapshot saved to memory_snapshot_2.7B_128_forward.pickle
Model: 2.7B, Context: 128, Mode: forward, BF16: False
Average Time: 0.0799 s
Std Dev: 0.0004 s
    \end{lstlisting}

\end{solution}


\begin{problem}[\code{nsys_profile}]
Nsight Systems Profiler (5 points)
\begin{questions}
    \item Profile your forward pass using \code{nsys}. Does the total time spent match your measurements from the Python standard library?
    \item Which CUDA kernel takes the most cumulative GPU time during the forward pass? How many times is it invoked? Is it the same for the backward pass?
    \item Besides matrix multiplications, what other kernels account for non-trivial runtime in the forward pass?
    \item Profile a complete training step with AdamW. How does the fraction of time spent on matrix multiplication change compared to inference?
    \item Compare the runtime vs. FLOPs difference for the softmax operation and matrix multiplications within the self-attention layer.
\end{questions}
\end{problem}

\begin{problem}[\code{mixed_precision_accumulation}]
    Mixed Precision Accumulation (1 point)

    Run the provided code involving FP32 and FP16 summations. Comment on the accuracy of the results for each case.

    \begin{lstlisting}[language=Python]
import torch

s = torch.tensor(0,dtype=torch.float32)
for i in range(1000):
    s += torch.tensor(0.01,dtype=torch.float32)
print(s)

s = torch.tensor(0,dtype=torch.float16)
for i in range(1000):
    s += torch.tensor(0.01,dtype=torch.float16)
print(s)

s = torch.tensor(0,dtype=torch.float32)
for i in range(1000):
    s += torch.tensor(0.01,dtype=torch.float16)
print(s)

s = torch.tensor(0,dtype=torch.float32)
for i in range(1000):
    x = torch.tensor(0.01,dtype=torch.float16)
    s += x.type(torch.float32)
print(s)        
    \end{lstlisting}

    output:
    \begin{lstlisting}
tensor(10.0001)
tensor(9.9531, dtype=torch.float16)
tensor(10.0021)
tensor(10.0021)
    \end{lstlisting}
\end{problem}

\begin{problem}[\code{benchmarking_mixed_precision}]
Mixed Precision Benchmarking (2 points)
\begin{questions}
    \item For the provided \code{ToyModel}, what are the data types of the parameters, FC1 output, LayerNorm output, logits, loss, and gradients when using FP16 autocasting?
    \item Which parts of Layer Normalization are sensitive to mixed precision? If using BF16 instead of FP16, do we still need to treat it differently?
    \item Modify your benchmarking script to support BF16 mixed precision. Compare results with full precision across different model sizes.
\end{questions}
\end{problem}

\begin{problem}[\code{memory_profiling}]
Memory Profiling (4 points)
\begin{questions}
    \item Profile the 2.7B model's forward pass and a full training step using \code{memory_viz}. Can you identify the stages based on the peaks?
    \item Report peak memory usage for context lengths 128, 256, and 512 for both forward pass and full training steps.
    \item Does mixed precision significantly affect peak memory usage?
    \item Calculate the size (in MB) of a single-precision activation tensor in the 2.7B model residual stream.
    \item Identify the largest allocations in the "Active Memory Timeline" and their origins in the code.
\end{questions}
\end{problem}

\begin{problem}[\code{pytorch_attention}]
Benchmarking PyTorch Attention (2 points)
\begin{questions}
    \item Benchmark your attention implementation across $d_{model} \in [16, 32, 64, 128]$ and $seq\_len \in [256, 1024, 4096, 8192, 16384]$. At what size do you encounter OOM errors? How does memory saved for backward scale with sequence length?
\end{questions}
\end{problem}

\begin{problem}[\code{torch_compile}]
JIT-Compiled Attention (2 points)
\begin{questions}
    \item Compare the performance of a \code{torch.compile} version of your attention vs. the uncompiled version.
    \item Compile your entire Transformer model. How does the performance of the forward pass and full training step change?
\end{questions}
\end{problem}

\section{Flash Attention Implementation}

\begin{problem}[\code{flash_forward}]
FlashAttention-2 Forward Pass (15 points)
\begin{questions}
    \item Implement a pure PyTorch (tiled) version of FlashAttention-2 forward pass to help with debugging.
    \item Write a Triton kernel for the FlashAttention-2 forward pass (Algorithm 1). Wrap it in a \code{torch.autograd.Function}.
    \item Add support for causal masking in the Triton kernel. Masked elements should have $-1\times 10^6$ added to the scores.
\end{questions}
\end{problem}

\begin{problem}[\code{flash_backward}]
FlashAttention-2 Backward Pass (5 points)
Implement the backward pass for FlashAttention-2 using \code{torch.compile} and the recomputation strategy (Equations 13-19).
\end{problem}
\begin{solution*}
    \paragraph{Experience} When to use \code{fp32} and when to use \code{bf16} / \code{fp16}?

    \subparagraph{\code{bf16} / \code{fp16} is better:}

    \begin{itemize}
        \item 全局内存的读写 (Global Memory I/O)

        对象： \code{Q, K, V, dO} 的 \code{tl.load}, 以及最终 \code{dK, dV} 的 \code{tl.store}, 突破 Memory-wall (显存带宽瓶颈). 使用 16-bit 数据可以将显存读写量直接减半. 

        \item 矩阵乘法的输入 (Tensor Core Inputs)

        对象： 传入 \code{tl.dot(a, b)} 的 a 和 b, 现代 GPU 的 Tensor Core 规定, 提高运算速度.

        Example:

        \begin{lstlisting}[language=Python]
# type of P_cast, dO are bf16 / fp16
# Incorrect: Triggers double casting and precision truncation
dV_acc += tl.dot(tl.trans(P_cast), dO).to(tl.float32) 
# Correct: Hardware directly outputs in 32-bit
dV_acc += tl.dot(tl.trans(P_cast), dO, out_dtype=tl.float32)       
        \end{lstlisting}
    \end{itemize}

    \subparagraph{Must use \code{fp32}:}

    \begin{itemize}
        \item 所有的累加器 (Accumulators)
        
        对象： 反向传播中的 \code{dK_acc}, \code{dV_acc}, 以及前向传播中的输出 \code{O_acc}, 容易导致截断误差.

        \item 指数、对数与除法运算 (Non-linear Math)
        
        对象： Softmax 核心计算, 如 \code{P=exp(S−L)}.  指数运算 $\exp$ 极其敏感, 输入端的微小精度丢失会被指数级放大. 

        \item 逐元素 (Element-wise)中间计算

        对象： 例如计算 \code{dS=P*(dP−D)}
    
    \end{itemize}



        

    


\end{solution*}


\section{Distributed Data Parallel Training}

\begin{problem}[\code{distributed_communication_single_node}]
Distributed Communication (5 points)
\begin{questions}
    \item Benchmark the \code{all-reduce} operation. Vary the backend (Gloo/NCCL), data size (1MB to 1GB), and number of processes (2, 4, 6).
\end{questions}
\end{problem}

\begin{problem}[\code{naive_ddp}]
Naïve DDP (5 points)
\begin{questions}
    \item Implement a script to perform DDP training by all-reducing individual parameter gradients. Verify correctness against single-process training.
\end{questions}
\end{problem}

\begin{problem}[\code{ddp_overlap_bucketed}]
Overlapping and Bucketing (8 points)
\begin{questions}
    \item Implement a DDP class that supports gradient bucketing (with a \code{bucket_size_mb} parameter) and overlaps communication with the backward pass using hooks.
\end{questions}
\end{problem}

\section{Optimizer State Sharding}

\begin{problem}[\code{optimizer_state_sharding}]
Optimizer State Sharding (15 points)
\begin{questions}
    \item Implement a sharded optimizer class that partitions optimizer states across ranks. Each rank should only update its own subset of parameters and then broadcast the updates.
\end{questions}
\end{problem}

\begin{problem}[\code{optimizer_state_sharding_accounting}]
Sharding Analysis (5 points)
\begin{questions}
    \item Profile peak memory usage with and without sharding. Report usage at initialization, before the optimizer step, and after.
    \item How does sharding affect training speed?
    \item Contrast your implementation with ZeRO Stage 1 in terms of memory and communication volume.
\end{questions}
\end{problem}



\end{document}